{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a2edcf-69a2-41a8-bf74-bf79187a8a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nForward propagation is where input data is fed through a network, in a forward direction, to generate an output. \\nThe data is accepted by hidden layers and processed, as per the activation function, and moves to the successive layer.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "'''\n",
    "Forward propagation is where input data is fed through a network, in a forward direction, to generate an output. \n",
    "The data is accepted by hidden layers and processed, as per the activation function, and moves to the successive layer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdce74c3-e111-4cef-ba18-307d3059df03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSteps to Perform Neural Network\\n\\nThere are three steps to perform in any neural network: We take the input variables and the above linear combination\\nequation of Z = W 0 + W 1X 1 + W 2X 2 + … + W nX n to compute the output or the predicted Y values, called the Y pred.\\nCalculate the loss or the error term.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q NO. 2 \n",
    "\"\"\"\n",
    "Steps to Perform Neural Network\n",
    "\n",
    "There are three steps to perform in any neural network: We take the input variables and the above linear combination\n",
    "equation of Z = W 0 + W 1X 1 + W 2X 2 + … + W nX n to compute the output or the predicted Y values, called the Y pred.\n",
    "Calculate the loss or the error term.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f88bc8-7ec4-46d4-8baa-356004fa51f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDuring forward propagation, pre-activation and activation take place at each hidden and output layer node of a neural network.\\nThe pre-activation function is the calculation of the weighted sum. The activation function is applied, based \\non the weighted sum, to make the neural network flow non-linearly using bias.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.3 :\n",
    "'''\n",
    "During forward propagation, pre-activation and activation take place at each hidden and output layer node of a neural network.\n",
    "The pre-activation function is the calculation of the weighted sum. The activation function is applied, based \n",
    "on the weighted sum, to make the neural network flow non-linearly using bias.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13674e9a-0c7b-46fe-b0cb-daa64fd73b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWeights set the standards for the neuron's signal strength. This value will determine the influence input data has\\non the output product. Biases give extra characteristics with a value of 1 that the neural network did not previously have.\\nThe neural network needs that extra information to efficiently propagate forward.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q NO. 4 :\n",
    "\"\"\"\n",
    "Weights set the standards for the neuron's signal strength. This value will determine the influence input data has\n",
    "on the output product. Biases give extra characteristics with a value of 1 that the neural network did not previously have.\n",
    "The neural network needs that extra information to efficiently propagate forward.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f06dc9-1f33-4078-ae7a-d18fc37c99d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe softmax function is used as the activation function in the output layer of neural network models \\nthat predict a multinomial probability distribution. That is, softmax is used as the activation function\\nfor multi-class classification problems where class membership is required on more than two class labels.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "'''\n",
    "The softmax function is used as the activation function in the output layer of neural network models \n",
    "that predict a multinomial probability distribution. That is, softmax is used as the activation function\n",
    "for multi-class classification problems where class membership is required on more than two class labels.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1877a86a-2d7d-4655-aa8c-65d467b85d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBackpropagation algorithms are used extensively to train feedforward neural networks in areas such as deep learning.\\nThey efficiently compute the gradient of the loss function with respect to the network weights.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q NO. 6 :\n",
    "'''\n",
    "Backpropagation algorithms are used extensively to train feedforward neural networks in areas such as deep learning.\n",
    "They efficiently compute the gradient of the loss function with respect to the network weights.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1baaa0da-080e-489c-8af9-6eeb964a8cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n∂ajk\\u200b∂alk+1\\u200b\\u200b=wjlk+1\\u200bg′(ajk\\u200b).\\nPlugging this into the above equation yields a final equation for the error term δ j k \\\\delta_j^k δjk\\u200b in the hidden layers,\\ncalled the backpropagation formula:\\nδ j k = ∑ l = 1 r k + 1 δ l k + 1 w j l k + 1 g ′ ( a j k ) = g ′ ( a j k ) ∑ l = 1 r k + 1 w j l k + 1 δ l k + 1 .\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "'''\n",
    "∂ajk​∂alk+1​​=wjlk+1​g′(ajk​).\n",
    "Plugging this into the above equation yields a final equation for the error term δ j k \\delta_j^k δjk​ in the hidden layers,\n",
    "called the backpropagation formula:\n",
    "δ j k = ∑ l = 1 r k + 1 δ l k + 1 w j l k + 1 g ′ ( a j k ) = g ′ ( a j k ) ∑ l = 1 r k + 1 w j l k + 1 δ l k + 1 .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ea8ff2-87f2-4ec8-9f5f-4ace3a0b0dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe algorithm is used to effectively train a neural network through a method called chain rule. \\nIn simple terms, after each forward pass through a network, backpropagation performs a backward pass while \\nadjusting the model's parameters (weights and biases).\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 8 :\n",
    "\"\"\"\n",
    "The algorithm is used to effectively train a neural network through a method called chain rule. \n",
    "In simple terms, after each forward pass through a network, backpropagation performs a backward pass while \n",
    "adjusting the model's parameters (weights and biases).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28e0e7f-9f7f-4a97-9838-4864d8858050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt is slow, all previous layers are locked until gradients for the current layer is calculated. \\nIt suffers from vanishing or exploding gradients problem. It suffers from overfitting & underfitting problem.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 9 :\n",
    "'''\n",
    "It is slow, all previous layers are locked until gradients for the current layer is calculated. \n",
    "It suffers from vanishing or exploding gradients problem. It suffers from overfitting & underfitting problem.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
